#!/usr/bin/env python3

from dotenv import load_dotenv, find_dotenv
load_dotenv(find_dotenv())

from arpy import ArchiveFormatError
from datetime import date
from lxml.builder import ElementMaker
from munch import Munch
from packaging import version as vcomp
from pydpkg import Dpkg
from string import Template
from urllib.request import urlopen
from urllib.parse import quote_plus as urlencode
import argparse
import boto3
import configparser
import github3 as github
import glob
import hashlib
import json
import lxml.etree as etree
import mimetypes
import os
import re
import shlex
import shutil
import socket
import subprocess
import sys
import tarfile
import tempfile
import textwrap
import toml
import chevron
from pushbullet import Pushbullet

from ruamel.yaml import YAML
yaml=YAML()

parser = argparse.ArgumentParser()
parser.add_argument('--verbose', action='store_true')
parser.add_argument('--bump', action='store_true')
parser.add_argument('--reload', action='store_true')
parser.add_argument('--rebuild', action='store_true')
Args = parser.parse_args()
if Args.reload or os.environ.get('REBUILD', '').strip() != '': Args.rebuild = True

#### global data ####
with open('config.toml') as f:
  Config = toml.load(f, _dict=Munch)

ArchMap = {
  'i686': 'i386',
  'x86_64': 'amd64'
}

def get_clients():
  clients = []
  for section, values in Config.items():
    if 'name' in values and 'comment' in values: clients.append(section)
  return clients
Clients = get_clients()

def pluralize(d, singular):
  plural = singular + 's'
  assert not (singular in d and plural in d)
  if singular in d:
    d[plural] = d[singular]
    del d[singular]
  if plural in d and type(d[plural]) != list:
    d[plural] = [ d[plural] ]

def load_mimeinfo():
  with open('mimeinfo.toml') as f:
    _mimeinfo = toml.load(f, _dict=Munch)
    for mi in _mimeinfo.values():
      for singular in ['mimetype', 'extension']:
        plural = singular + 's'
        assert not (singular in mi and plural in mi)
        if singular in mi:
          mi[plural] = mi[singular]
          del mi[singular]
        if plural in mi and type(mi[plural]) != list:
          mi[plural] = [ mi[plural] ]
      
      if 'extensions' in mi:
        mi['extensions'] = [ext[1:] if ext[0] == '.' else ext for ext in mi['extensions']]
  return _mimeinfo
MimeInfo = load_mimeinfo()

def load_esr_deps():
  deps = []
  for dep in os.popen('apt-cache depends firefox-esr').read().split('\n'):
    dep = dep.strip()
    if not dep.startswith('Depends:'): continue
    dep = dep.split(':')[1].strip()
    if dep == 'lsb-release': continue # why should it need this?
    if 'gcc' in dep: continue #43
    deps.append(dep)
  return deps
if not Args.bump:
  ESR = load_esr_deps()

#### convenience functions ####

#from urllib.request import urlretrieve
def urlretrieve(url, path):
  run(f'curl -s -L -o {shlex.quote(path)} {shlex.quote(url)}')

class Repository:
  def __init__(self):
    self.rebuild = 'Packages' not in self._assets # deleting 'Packages' forces re-uploading of everything
    if self.rebuild:
      for asset in self._assets.values():
        self.delete(asset)
      self._assets = {}
    self.repo = os.environ.get('GITHUB_REPOSITORY')

  def unchanged(self):
    published = set([asset for asset in self._assets.keys() if asset.endswith('.deb')])
    available = set([os.path.basename(deb.deb) for deb in Deb.all()])
    return published == available

  def verify(self):
    print('\n## verifying repo hashes/sizes')
    with open('repo/Packages') as f:
      packages = f.read()
      for pkg in packages.split('\n\n'):
        pkg = pkg.strip()
        if pkg == '': continue
        if Args.verbose: print(pkg)
        deb = Munch.fromDict(dict([tuple([k.strip() for k in kv.split(':', 1)]) for kv in pkg.split('\n')]))
        print('\n ', os.path.basename(deb.Filename))

        expected = Munch(MD5sum=hashlib.md5(), SHA1=hashlib.sha1(), SHA256=hashlib.sha256(), SHA512=hashlib.sha512())
        with open(os.path.join('repo', deb.Filename), 'rb') as f:
          for chunk in iter(lambda: f.read(4096), b""):
            for h in expected.values():
              h.update(chunk)
        expected = Munch.fromDict({ k: h.hexdigest() for k, h in expected.items() })
        expected.Size = str(os.path.getsize(os.path.join('repo', deb.Filename)))
        for k in expected.keys():
          assert expected[k] == deb[k], f'{k} mismatch, expected {expected[k]}, found {deb[k]}'
          print(f'  = {k}: {deb[k]}')
    print('  verify ready')

  def page(self, md='index', list_debs=False, as_html=False):
    data = { 'url': self.url }
    for client in Clients:
      data[client] = Deb.select(client=client)[-1].version
    data['debs'] = { 'deb': os.path.basename(deb) for deb in sorted(glob.glob('repo/*.deb')) }

    if self.repo:
      with open('.github/workflows/publish.yaml') as f:
        workflow = yaml.load(f)
        data['badge'] = f'https://github.com/{self.repo}/workflows/{urlencode(workflow["name"])}/badge.svg?branch=master'
    else:
      data['badge'] = ''

    with open(f'{md}.md') as f:
      index = f.read()
      if list_debs: index += '\n\n---\n\n{{#debs}}{{deb}}{{/debs}}'
      index = chevron.render(index, data)
      if as_html: index = markdown.markdown(index)
      return index

class GitHub(Repository):
  def __init__(self, config):
    self.repo = os.environ['GITHUB_REPOSITORY']
    self.service = 'github'
    self.url = f'https://github.com/{self.repo}/releases/download/{config.release}'

    gh = github.GitHub(token=os.getenv('GITHUB_TOKEN'), session=github.session.GitHubSession(default_read_timeout=60))
    repo = gh.repository(*self.repo.split('/'))

    self._release = repo.release_from_tag(config.release)
    self._assets = { asset.name: asset for asset in self._release.assets() }

    super().__init__()

  def delete(self, asset):
    asset.delete()

  def get(self, name):
    if not name.endswith('.deb'): return False

    asset = self._assets.get(os.path.basename(name))
    if asset:
      os.makedirs(os.path.dirname(name), exist_ok=True)
      urlretrieve(asset.browser_download_url, name)
      return True
    return False

  def restore(self):
    os.makedirs('repo', exist_ok=True)
    for asset in self._release.assets():
      urlretrieve(asset.browser_download_url, os.path.join('repo', asset.name))

  def publish(self):
    print(f'\n## publishing repo to github')

    if building := self.page(md='building'):
      print(f'  updating release {self._release.name} description')
      try:
        self._release.edit(body=building)
      except Exception as e:
        print(e)

    published = []
    for asset in self._release.assets():
      if not os.path.exists(f'repo/{asset.name}'):
        print('  deleting obsolete asset:', asset.name)
        asset.delete()
      elif asset.name.endswith('.deb') and not Args.rebuild:
        published.append(asset.name)
      else:
        print('  refreshing asset:', asset.name)
        asset.delete()

    for asset in sorted(glob.glob('repo/*')):
      if os.path.basename(asset) in published: continue

      content_type = mimetypes.guess_type(asset)[0] or 'application/octet-stream'
      print(f'  uploading {asset} ({content_type})')
      with open(asset, 'rb') as f:
        self._release.upload_asset(
          asset=f,
          name=os.path.basename(asset),
          content_type=content_type
        )

    if index := self.page(md='index'):
      print(f'  updating release {self._release.name} description')
      try:
        self._release.edit(body=index)
      except Exception as e:
        print(e)

class Local(Repository):
  def __init__(self, config):
    self.service = 'local'
    self.url = 'http://localhost'

    self._assets = {}
    for asset in glob.glob('repo/*'):
      self._assets[os.path.basename(asset)] = asset
    super().__init__()

  def delete(self, asset):
    os.remove(f'repo/{asset}')

  def get(self, asset):
    return os.path.exists(os.path.join('repo', asset))

  def publish(self):
    print(f'\n## nothing to publish')

  def restore(self):
    pass

class S3(Repository):
  def __init__(self, config):
    self.service = 's3'
    self.url = f'https://{config.bucket}.s3.{config.region}.amazonaws.com'

    s3 = boto3.resource('s3')
    self._bucket = s3.Bucket(name=config.bucket)
    self._assets = { asset.key: asset for asset in self.bucket.objects.all() }

    super().__init__()

  def delete(self, asset):
    self._bucket.delete_key(asset)
    
  def get(self, name):
    if not name.endswith('.deb'): return False

    try:
      for asset in self.bucket.objects.all():
        if os.path.basename(asset.key) == os.path.basename(name):
          self.bucket.download_file(os.path.basename(name), name)
          return True
    except:
      print('  error: could not get {os.path.basename(name)}')
      if os.path.exists(name): os.remove(name)

    return False

  def publish(self):
    print(f'\n## publishing repo to S3 bucket')
    published = []
    for asset in self.bucket.objects.all():
      if not os.path.exists(f'repo/{asset.key}'):
        print('  deleting obsolete asset:', asset.key)
        self.bucket.delete_key(asset)
      elif asset.key.endswith('.deb') and not Args.rebuild:
        published.append(asset.key)
      else:
        print('  refreshing asset:', asset.key)

    for asset in sorted(glob.glob('repo/*')):
      if os.path.basename(asset) in published: continue

      content_type = mimetypes.guess_type(asset)[0] or 'application/octet-stream'
      print(f'  uploading {asset} ({content_type})')
      self.bucket.upload_file(asset, os.path.basename(asset))

def load_repo():
  branch = os.environ.get('GITHUB_REF', '')
  branch = branch.split('/')[-1] if branch.startswith('refs/heads/') else f'@{socket.gethostname()}'
  repo = None
  print(f'\n## branch = {branch}')
  if branch and branch in Config:
    for hosting, config in [(h.lower(), data) for h, data in Config[branch].items() if h.lower() in ('local', 'github', 's3')]:
      if hosting == 'github':
        print('\n## publish target: Github release')
        repo = GitHub(config)
      elif hosting == 's3':
        print('\n## publish target: S3 bucket')
        repo = S3(config)
      elif hosting == 'local':
        print('\n## publish target: local')
        repo = Local(config)
      break

  if not repo:
    print(f'  no repo found for {branch}')
  return repo
Repo = load_repo()

def run(cmd, echo=True):
  if echo: print('  $', cmd)
  output = textwrap.indent(subprocess.check_output(cmd, shell=True, stderr=subprocess.STDOUT).decode('utf-8'), '    ')
  if echo: print(output)

class Open():
  def __init__(self, path, mode='r', fmode=None):
    if 'w' in mode or 'a' in mode: os.makedirs(os.path.dirname(path), exist_ok=True)
    self.path = path
    self.mode = fmode
    self.f = open(path, mode)
  def __enter__(self):
    return self.f
  def __exit__(self, exc_type, exc_value, exc_traceback):
    self.f.close()
    if self.mode is not None:
      os.chmod(self.path, self.mode)

def load(url,parse_json=False):
  response = urlopen(url).read()
  if type(response) is bytes: response = response.decode('utf-8')
  if parse_json:
    return json.loads(response, object_hook=Munch.fromDict)
  else:
    return response

class Version:
  def __init__(self, deb, *args):
    self.deb  = deb
  def __lt__(self, other):
    if self.deb.client == other.deb.client:
      return vcomp.parse(self.deb.version.replace('m', '.')) < vcomp.parse(other.deb.version.replace('m', '.'))
    else:
      return self.deb.client < other.deb.client
  def __gt__(self, other):
    if self.deb.client == other.deb.client:
      return vcomp.parse(self.deb.version.replace('m', '.')) > vcomp.parse(other.deb.version.replace('m', '.'))
    else:
      return self.deb.client > other.deb.client
  def __eq__(self, other):
    return self.deb.client == other.deb.client and vcomp.parse(self.deb.version.replace('m', '.')) == vcomp.parse(other.deb.version.replace('m', '.'))
  def __le__(self, other):
    if self.deb.client == other.deb.client:
      return vcomp.parse(self.deb.version.replace('m', '.')) <= vcomp.parse(other.deb.version.replace('m', '.'))
    else:
      return self.deb.client <= other.deb.client
  def __ge__(self, other):
    if self.deb.client == other.deb.client:
      return vcomp.parse(self.deb.version.replace('m', '.')) >= vcomp.parse(other.deb.version.replace('m', '.'))
    else:
      return self.deb.client >= other.deb.client
  def __ne__(self, other):
    return self.deb.client != other.deb.client and vcomp.parse(self.deb.version.replace('m', '.')) != vcomp.parse(other.deb.version.replace('m', '.'))

class Deb:
  __debs = []

  @classmethod
  def select(cls, version = None, patch = None, client = None, arch=None, aptarch=None, beta=None):
    def equal(deb):
      return all((given is None or given == found) for given, found in [(version, deb.version), (patch, deb.version + deb.apt.patch), (client, deb.client), (arch, deb.arch), (aptarch, deb.apt.arch), (beta, deb.beta)])
    return sorted([deb for deb in cls.__debs if equal(deb)], key=Version)

  @classmethod
  def all(cls):
    return cls.select()

  @classmethod
  def rebuilt(cls):
    return any(deb for deb in cls.__debs if deb.rebuilt)

  def __init__(self, client, version, arch):
    Deb.__debs.append(self)

    self.rebuilt = False

    self.client = client
    self.version = version
    self.arch = arch
    self.beta = version.startswith('beta-')

    # set download URL
    self.url = {
      'zotero':       f'https://www.zotero.org/download/client/dl?channel=release&platform=linux-{arch}&version={version}',
      'zotero-beta':  f'https://www.zotero.org/download/client/dl?channel=beta&platform=linux-{arch}',

      'jurism':       f'https://github.com/Juris-M/assets/releases/download/client%2Frelease%2F{version}/Jurism-{version}_linux-{arch}.tar.bz2',
      'jurism-beta':  f'https://our.law.nagoya-u.ac.jp/jurism/dl?channel=beta&platform=linux-{arch}',
    }[f'{self.client}{"-beta" if self.beta else ""}']

  @property
  def deb(self):
    return f'repo/{self.client}{self.apt.postfix}_{self.apt.version}{self.apt.patch}_{self.apt.arch}.deb'

  @property
  def tarball(self):
    return os.path.join(self.client, self.arch, self.version + '.tar.bz2')

  @property
  def apt(self):
    if self.beta:
      apt = Munch(
        postfix='-beta',
        version=self.version.replace('beta-', '').replace('-', '.'),
        patch='',
        arch=ArchMap[self.arch]
      )
    else:
      apt = Munch(
        postfix='',
        version=self.version,
        patch='',
        arch=ArchMap[self.arch]
      )
      if 'patch' in Config[self.client] and self.version in Config[self.client].patch:
        apt.patch = '-' + str(Config[self.client].patch[self.version])
    return apt
    
  def build(self):
    if not 'patch' in Config[self.client]: Config[self.client].patch = Munch()

    print(f'\n## building {self.deb}')
    self.rebuilt = True

    exists = None
    if Args.rebuild:
      print('  forced rebuild')
    elif os.path.exists(self.deb):
      exists = f'  {self.deb} already exists'
    elif Repo and Repo.get(self.deb):
      exists = f'  {self.deb} re-fetched'
    if exists:
      try:
        # for some bloody reason Github frequently hands us a JSON description of the asset rather than the asset itself
        run(f'dpkg --info {self.deb}', echo=False)
        print(exists)
        return
      except ArchiveFormatError:
        print(exists + ', but is corrupted')
        if os.path.exists(self.deb): os.remove(self.deb)

    if not os.path.exists(self.tarball) or Args.reload:
      os.makedirs(os.path.dirname(self.tarball), exist_ok=True)
      print('  downloading', self.tarball)
      urlretrieve(self.url, self.tarball)

    if os.path.exists('build'): shutil.rmtree('build')
    os.makedirs('build')
  
    print(f'  unpacking {self.tarball}')
    tar = tarfile.open(self.tarball)
    for member in tar.getmembers():
      if not member.isreg(): continue
      member.name = re.sub(r'^.+?\/', '', member.name) # strip leading directory
  
      if member.name in ['zotero.desktop', 'jurism.desktop', 'active-update.xml', 'precomplete', 'removed-files', 'updates', 'updates.xml']:
        continue
  
      tar.extract(member, f'build/usr/lib/{self.client}{self.apt.postfix}')
    tar.close()
  
    print(f'  disable auto-update')
    with Open(f'build/usr/lib/{self.client}{self.apt.postfix}/defaults/pref/local-settings.js', 'a') as ls, Open(f'build/usr/lib/{self.client}{self.apt.postfix}/mozilla.cfg', 'a') as cfg:
      # enable mozilla.cfg
      if ls.tell() != 0: print('', file=ls)
      print('pref("general.config.obscure_value", 0); // only needed if you do not want to obscure the content with ROT-13', file=ls)
      print('pref("general.config.filename", "mozilla.cfg");', file=ls)

      # disable auto-update
      if cfg.tell() == 0:
        print('//', file=cfg)
      else:
        print('', file=cfg)
      print('lockPref("app.update.enabled", false);', file=cfg)
      print('lockPref("app.update.auto", false);', file=cfg)
  
    print(f'  write launcher entry')
    with Open(f'build/usr/share/applications/{self.client}{self.apt.postfix}.desktop', 'w') as f:
      desktop = configparser.RawConfigParser()
      desktop.add_section('Desktop Entry')
      desktop.optionxform=str
      desktop.set('Desktop Entry', 'Name', Config[self.client].name + self.apt.postfix.replace('-', ' '))
      desktop.set('Desktop Entry', 'Comment', Config[self.client].comment)
      desktop.set('Desktop Entry', 'Exec', f'/usr/lib/{self.client}{self.apt.postfix}/{self.client} --url %u')
      desktop.set('Desktop Entry', 'Icon', f'/usr/lib/{self.client}{self.apt.postfix}/chrome/icons/default/default256.png')
      desktop.set('Desktop Entry', 'Type', 'Application')
      desktop.set('Desktop Entry', 'Categories', Config[self.client].categories)
      desktop.set('Desktop Entry', 'StartupNotify', 'true')
      desktop.set('Desktop Entry', 'MimeType', ';'.join([mt for mi in MimeInfo.values() for mt in mi.mimetypes]))
      desktop.write(f, space_around_delimiters=False)

    print(f'  update mime info')
    with Open(f'build/usr/share/mime/packages/{self.client}{self.apt.postfix}.xml', 'wb') as f:
      E = ElementMaker(
        namespace='http://www.freedesktop.org/standards/shared-mime-info',
        nsmap={
          None : 'http://www.freedesktop.org/standards/shared-mime-info',
          'xml': 'http://www.w3.org/XML/1998/namespace',
        }
      )
      _mimetypes = []
      MIMETYPE = getattr(E, 'mime-type')
      MIMEINFO = getattr(E, 'mime-info')
      for name, mi in MimeInfo.items():
        if not 'extensions' in mi: continue

        children = [E.comment(name)]
        for k, v in mi.items():
          if len(k) == 2:
            children.append(E.comment(v, **{'{http://www.w3.org/XML/1998/namespace}lang': k}))
        for ext in mi.get('extensions', []):
          children.append(E.glob(pattern=f'*.{ext}'))
        for mt in mi.mimetypes[1:]:
          children.append(E.alias(type=mt))
        _mimetypes.append(MIMETYPE(*children, type=mi.mimetypes[0]))
      f.write(etree.tostring(MIMEINFO(*_mimetypes), pretty_print=True, xml_declaration=True, encoding='utf-8'))

    print(f'  write build control file')
    with Open('build/DEBIAN/control', 'w') as f:
      dependencies = ', '.join(sorted(list(set(Config[self.client].dependencies + ESR))))
      print(f'Package: {self.client}{self.apt.postfix}', file=f)
      print(f'Architecture: {self.apt.arch}', file=f)
      print(f'Depends: {dependencies}'.strip(), file=f)
      print(f'Maintainer: {Config.maintainer.email}', file=f)
      print(f'Section: {Config[self.client].section}', file=f)
      print('Priority: optional', file=f)
      print(f'Version: {self.apt.version}{self.apt.patch}', file=f)
      print(f'Description: {Config[self.client].description}', file=f)

    #with Open('build/DEBIAN/postinst', 'w') as postinst, open('postinst') as tmpl:
    #  postinst.write(chevron.render(tmpl.read(), { 'client': self.client + self.apt.postfix }))
    #os.chmod('build/DEBIAN/postinst', 0o775)

    os.makedirs('build/usr/local/bin')
    os.symlink(f'/usr/lib/{self.client}{self.apt.postfix}/{self.client}', f'build/usr/local/bin/{self.client}{self.apt.postfix}')

    os.makedirs('repo', exist_ok=True)
    run(f'fakeroot dpkg-deb --build -Zgzip build {self.deb}')
    run(f'dpkg-sig -k {Config.maintainer.gpgkey} --sign builder {self.deb}')

#### deb builder ####

class Builder:
  def __init__(self):
    self.gather()
    self.cleanup()
    self.patch()
    self.build()
    self.trim()
    self.publish()

  def gather(self):
    # gather .debs to build
    for client in Clients:
      if client == 'zotero':
        versions = [release.version for release in load('https://www.zotero.org/download/client/manifests/release/updates-linux-x86_64.json', parse_json=True)]
      else:
        versions = []
        # jurism puts out new version in a frenzied burst -- only keep the last m-version
        for version in [release for release in load('https://github.com/Juris-M/assets/releases/download/client%2Freleases%2Fincrementals-linux/incrementals-release-linux').split('\n') if release != '']:
          if len(versions) == 0:
            versions.append(version)
          elif version.split('m')[0] == versions[-1].split('m')[0]:
            versions[-1] = version
          else:
            versions.append(version)
      #versions = versions[-1:]
      versions.append(f'beta-{date.today().isoformat()}')

      for version in versions:
        for arch in ArchMap.keys():
          Deb(client, version, arch)

  def cleanup(self):
    print('\n## pre-build cleanup')
    # cleanup interrupted build
    for deb in glob.glob('repo/*.deb.*'):
      os.remove(deb)

    # cleanup leftover tarballs
    for client in Clients:
      for tarball in glob.glob(f'{client}/*/*.tar.bz2'):
        arch = os.path.basename(os.path.dirname(tarball))
        version = os.path.basename(tarball).replace('.tar.bz2', '')
        if not any(deb for deb in Deb.select(client=client, version=version, arch=arch)):
          print('  removing', tarball)
          os.remove(tarball)

  def patch(self):
    changed = False
    for client in Clients:
      if not 'patch' in Config[client]: continue
      for version in list(Config[client].patch.keys()):
        if len(Deb.select(client=client, version=version, beta=False)) == 0:
          Config[client].patch.pop(version)
          changed = True
    if changed:
      with open('config.toml', 'w') as f:
        toml.dump(Config, f)
    # bump patch level
    if Args.bump:
      for client in Clients:
        debs = Deb.select(client=client, beta=False)
        if not 'patch' in Config[client]:
          Config[client].patch = Munch()
        if not debs[-1].version in Config[client].patch:
          Config[client].patch[debs[-1].version] = 0
        Config[client].patch[debs[-1].version] += 1
      with open('config.toml', 'w') as f:
        toml.dump(Config, f)
      sys.exit()

  def build(self):
    # build and download as needed

    if not Args.rebuild and Repo and Repo.unchanged():
      print('\n## repo up to date')
      sys.exit()
    if Repo and Repo.rebuild: Args.rebuild = True

    for deb in Deb.all():
      try:
        deb.build()
      except Exception as e:
        if deb.beta and 'PUSHBULLET_API_KEY' in os.environ:
          # just ignore it. It doesn't look like JM betas will return
          pass
          # Pushbullet(os.environ['PUSHBULLET_API_KEY']).push_note(f'{deb.client} beta failed to build', 'Possibly a download error')
        else:
          raise e

  def trim(self):
    # cleanup leftover .debs
    print('\n## Remove obsolete .debs')
    for deb in glob.glob('repo/*.deb'):
      client, version, aptarch = os.path.splitext(os.path.basename(deb))[0].split('_')[:3]
      if client.endswith('-beta'):
        version = 'beta-' + version.replace('.', '-')
        client = client.replace('-beta', '')
      if not any(d for d in Deb.select(client=client, patch=version, aptarch=aptarch)):
        if os.path.exists('repo/Packages'): os.remove('repo/Packages')
        print('  removing', deb)
        os.remove(deb)

  def publish(self):
    if Deb.rebuilt or not os.path.exists('repo/Packages'):
      print(f'\n## preparing repo')
      run(f'gpg --armor --export {Config.maintainer.gpgkey} > repo/deb.gpg.key')
      run(f'cd repo && apt-ftparchive packages . > Packages')
      run(f'bzip2 -kf repo/Packages')
      run(f'cd repo && apt-ftparchive release . > Release')
      run(f'gpg --yes -abs -u {Config.maintainer.gpgkey} -o repo/Release.gpg --digest-algo sha256 repo/Release')
      run(f'gpg --yes -abs -u {Config.maintainer.gpgkey} --clearsign -o repo/InRelease --digest-algo sha256 repo/Release')

      if Repo:
        with open('repo/install.sh', 'w') as install, open('install.sh') as tmpl:
          install.write(chevron.render(tmpl.read(), { 'url': Repo.url }))
      else:
        if os.path.exists('repo/install.sh'): os.remove('repo/install.sh')

      if Repo:
        Repo.publish()
        print(f'::set-output name=url::{Repo.url}')
        print('::set-output name=rebuilt::true')

        Repo.verify()
        Repo.restore()
        Repo.verify()

      else:
        print(f'\n## not publishing repo: no repo specified')
    else:
      print(f'\n## not publishing repo: nothing rebuilt')

# kick off build
try:
  Builder()
except subprocess.CalledProcessError as e:
  print(textwrap.indent(e.output.decode('utf-8'), '    '))
  sys.exit(1)
